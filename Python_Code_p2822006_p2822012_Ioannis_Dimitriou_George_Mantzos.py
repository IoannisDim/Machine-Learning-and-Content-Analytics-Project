# -*- coding: utf-8 -*-
"""20210902ML_Project (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIWcEH3b8PO4cCXsyrgQVuBJlrwwy1-c

#Athens University of Economics and Business
##MSc in Business Analytics (Part-Time 2020-2021)

###Machine Learning And Content Analytics

####George Mantzos (p2822012)
####Ioannis Dimitriou (p2822006)

#####August 2021

##**Connect to Google Drive in order to retrieve the uploaded data**

The data are uploaded and can be reached in the following link:

https://drive.google.com/drive/folders/1_N2fn1KJj5JAnOMybS-brhghULbOTXUB?usp=sharing
"""

from google.colab import drive
drive.mount('/content/drive')

"""There are Data about:


1.   **Argument Labeling**:     *'dataset_aueb_argument_v3.json'*
2.   **Structure Labeling**:    *'dataset_aueb_structure_v2.json'*
3.   **Citance Labeling**:      *'dataset_aueb_citations_v2.json'*

#**1. Intuitive Baseline**
### **a. Exploratory Data Analysis**
#### Argument Labeling Data

# Load Dataset

* The resulting dataset has one abstract per row. 
* The sentences and the labels are list items.
* For the Argument Labeling we have more data from the sample one, which were provided, so they will also be used.
* Both datasets will be loaded individually and they will be merged when they have the appropriate form.
"""

import json
import pandas as pd

label2id = {
    'NONE': 0,
    'EVIDENCE': 1,
    'CLAIM': 2}

def load_corpus(path, label_mapping=None):
    with open(path) as fp:
        corpus = json.load(fp)

    documents, texts, labels = [], [], []
    for abstract in corpus:
        documents.append(abstract)
        texts.append(corpus[abstract]['sentences'])
        if isinstance(label_mapping, dict):
            labels.append(
                [label_mapping[str(l).upper()]
                    for l in corpus[abstract]['labels']])
        else:
            labels.append([str(l).upper() for l in corpus[abstract]['labels']])

    assert len(texts) == len(labels)
    data = pd.DataFrame(
        zip(documents, texts, labels),
        columns=['document', 'sentences', 'labels'])

    return data

data_argument1 = load_corpus('/content/drive/MyDrive/ML_PROJECT/Data/dataset_aueb_argument_v3.json')
print(f'Dataset length: {len(data_argument1)} abstracts')
data_argument1.sample(5)

data_argument2 = load_corpus('/content/drive/MyDrive/ML_PROJECT/Data/dataset.json')
print(f'Dataset length: {len(data_argument2)} abstracts')
data_argument2.sample(5)

"""## Split Documents
For the cases we want the sentences separated, the following splits the documents. I keep the same document index in a new column in order to re-group the sentences to a document (e.g., after predictions).
"""

#@title Split to sentences
sentences1 = data_argument1['sentences'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'sentences': 'sentence'})
sentences1.head()

sentences2 = data_argument2['sentences'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'sentences': 'sentence'})
sentences2.head()

#@title and the corresponding labels
labels1 = data_argument1['labels'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'labels': 'label'})
labels1.head()

#@title and the corresponding labels
labels2 = data_argument2['labels'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'labels': 'label'})
labels2.head()

#@title Regroup document using doc_id
doc0_sents1 = sentences1.query('doc_id==0')['sentence']
doc0_labels1 = labels1.query('doc_id==0')['label']
for label, sent in zip(doc0_labels1, doc0_sents1):
    print(f'{str(label):5s} {sent}')

#@title Regroup document using doc_id
doc0_sents2 = sentences2.query('doc_id==0')['sentence']
doc0_labels2 = labels2.query('doc_id==0')['label']
for label, sent in zip(doc0_labels2, doc0_sents2):
    print(f'{str(label):5s} {sent}')

"""* The two datasets are now in form that can be merged into one dataframe.
* Before doing that we have to **add the total amount of documents of the first dataset to each Document ID of the second one.** 
"""

print(f'Dataset 1 length: {len(data_argument1)} abstracts')
print(f'Dataset 2 length: {len(data_argument2)} abstracts')
data_arg1_len = len(data_argument1)
sentences2.doc_id = sentences2.doc_id + data_arg1_len
labels2.doc_id = labels2.doc_id + data_arg1_len

print(f'Dataset 1 Minimum Document ID: {sentences1.doc_id.min()}')
print(f'Dataset 1 Maximum Document ID: {sentences1.doc_id.max()}')
print(f'Dataset 2 Minimum Document ID: {sentences2.doc_id.min()}')
print(f'Dataset 2 Maximum Document ID: {sentences2.doc_id.max()}')

"""* Also the labels datasets have a slight difference which should be corrected before merging tha datasets into one.
* The first label dataseT has as labels **"CLAIM", "EVIDENCE", "NEITHER"**.
* The second label dataseT has as labels **"CLAIM", "EVIDENCE", "NONE"**.
* So, we are going to convert the "NONE" label to "NEITHER".
"""

labels2.label[labels2['label'] == 'NONE'] = 'NEITHER'
labels2.label[labels2['label'] == 'NONE']

#@title First for the Sentences
sentences = sentences1.append(sentences2).reset_index(drop = True)
sentences

#@title Then for the Labels
labels = labels1.append(labels2).reset_index(drop = True)
labels

"""> Check that the merging was successful. """

print(f'Total Number of "NEITHER" in First Dataset: {labels1.label[labels1["label"] == "NEITHER"].value_counts()[0]}')
print(f'Total Number "NEITHER" in Second Dataset: {labels2.label[labels2["label"] == "NEITHER"].value_counts()[0]}')
print(f'Total Number of "NEITHER" in both Datasets: {labels1.label[labels1["label"] == "NEITHER"].value_counts()[0] +\
                                                     labels2.label[labels2["label"] == "NEITHER"].value_counts()[0]}')
print()
print(f'Total Number of "NEITHER" in the new Dataset: {labels.label[labels["label"] == "NEITHER"].value_counts()[0]}')

print(f'Total Number of "CLAIM" in First Dataset: {labels1.label[labels["label"] == "CLAIM"].value_counts()[0]}')
print(f'Total Number "CLAIM" in Second Dataset: {labels2.label[labels["label"] == "CLAIM"].value_counts()[0]}')
print(f'Total Number of "CLAIM" in both Datasets: {labels1.label[labels1["label"] == "CLAIM"].value_counts()[0] + labels2.label[labels2["label"] == "CLAIM"].value_counts()[0]}')
print()
print(f'Total Number of "CLAIM" in the new Dataset: {labels.label[labels["label"] == "CLAIM"].value_counts()[0]}')

print(f'Total Number of "EVIDENCE" in First Dataset: {labels1.label[labels["label"] == "EVIDENCE"].value_counts()[0]}')
print(f'Total Number "EVIDENCE" in Second Dataset: {labels2.label[labels["label"] == "EVIDENCE"].value_counts()[0]}')
print(f'Total Number of "EVIDENCE" in both Datasets: {labels1.label[labels1["label"] == "EVIDENCE"].value_counts()[0] +\
                                                      labels2.label[labels2["label"] == "EVIDENCE"].value_counts()[0]}')
print()
print(f'Total Number of "EVIDENCE" in the new Dataset: {labels.label[labels["label"] == "EVIDENCE"].value_counts()[0]}')

"""# Data Cleansing
## Title Removal
> * First of all, we know that the first sentence of each Document is their title.
> * It can produce some bias to our model, as it will always have the label **'NEITHER'**. 
> * So there will be 2585 such labels more than they actually exist in the dataset.
> * In order to do so, we will add 3 more columns in both dataframes.
>> 1.   **Sentences_Per_Doc**: Showing the number of sentences in each Document.
>> 2.   **Sentences_Id**: Actually the index of each Sentence.
>> 3.   **In_Doc_Sentence_Id**: The internal Id for any sentence in each Document, taking values from 0 to the "Sentences_Per_Doc" value of each document.
> * After that, all sentences and labels having 'In_Doc_Id' equal to zero(0) will be dropped. 

For space and time economy, this procedure will take place in parallel for sentences and labels.






"""

#@title Sentences_Per_Abstract
sentences_per_abstract = sentences.doc_id.value_counts()
sentences_per_abstract = sentences_per_abstract.sort_index()
sentences_per_abstract

import pandas as pd
d = {'doc_id': sentences_per_abstract.index, 
     'Sentences_Per Doc': sentences_per_abstract}
Sentences_Per_Abstract = pd.DataFrame(data = d)
Sentences_Per_Abstract

sentences = sentences.merge(Sentences_Per_Abstract[['doc_id', 'Sentences_Per Doc']], how = 'inner',
                            left_on = 'doc_id', right_on = 'doc_id')
 sentences

labels = labels.merge(Sentences_Per_Abstract[['doc_id', 'Sentences_Per Doc']], how = 'inner',
                            left_on = 'doc_id', right_on = 'doc_id')
labels

#@title Sentence_Id
sentences['Sentence_Id'] = sentences.index
sentences

labels['Sentence_Id'] = labels.index
labels

""">For The In_Doc_Id variable we are going to assign the values of the first document with doc_id = 0 and the others will be computed."""

print(f'The number of sentences of the first Document is: {sentences_per_abstract[0]}')

#@title In_Doc_Sentence_Id
sentences['In_Doc_Sentence_ID'] = ''
sentences['In_Doc_Sentence_ID'][0:17] = list(sentences['Sentence_Id'][0:17])

labels['In_Doc_Sentence_ID'] = ''
labels['In_Doc_Sentence_ID'][0:17] = list(labels['Sentence_Id'][0:17])
labels.head(18)

sentences_count = labels.label.count() #30184
i = 17
j = 0
while (i < sentences_count):
  
  if sentences['doc_id'][i-1] < sentences['doc_id'][i]:
    j = 0
    sentences['In_Doc_Sentence_ID'][i] = j

  elif sentences['doc_id'][i-1] == sentences['doc_id'][i]:
    sentences['In_Doc_Sentence_ID'][i] = j
  
  j = j + 1  
  i += 1

sentences

i = 17
j = 0
while (i < sentences_count):
  
  if labels['doc_id'][i-1] < labels['doc_id'][i]:
    j = 0
    labels['In_Doc_Sentence_ID'][i] = j

  elif labels['doc_id'][i-1] == labels['doc_id'][i]:
    labels['In_Doc_Sentence_ID'][i] = j
  
  j = j + 1  
  i += 1

labels

"""> Keep only the sentences and their corresponding label which have 'In_Doc'"""

#wt correspond to without title

sentences_wt = sentences[sentences['In_Doc_Sentence_ID'] != 0].reset_index(drop=True)
sentences_wt

labels_wt = labels[labels['In_Doc_Sentence_ID'] != 0].reset_index(drop=True)
labels_wt

"""> After Dropping the title of each Document, we have to reduce the value for the 'Sentences_Per_Doc' by 1. """

sentences_wt['Sentences_Per Doc'] = sentences_wt['Sentences_Per Doc'] - 1
sentences_wt.head()

sentences_wt.head()

labels_wt['Sentences_Per Doc'] = labels_wt['Sentences_Per Doc'] - 1
labels_wt.head()

"""## Extreme Outliers in 'Sentences_Per_Doc' Detection and Removal.
> * Before Splitting out our Data to Training and Test Set and, we are going to explore our Data and see if there extreme values in how much sentences a Document has.
> * Extreme outliers will probably conclude to bias in our argument labeling, as they will have much more 'NEITHER' labels than the majority and may also have much more of the rest labels.
> * But let's explore the distribution of that variable firstly.
"""

print(f'Minimum Number of Sentences Per Abstract: {labels_wt["Sentences_Per Doc"].min()}')
print(f'Maximum Number of Sentences Per Abstract: {labels_wt["Sentences_Per Doc"].max()}')
print(f'Average Number of Sentences Per Abstract: {labels_wt["Sentences_Per Doc"].mean()}')
print(f'Median Number of Sentences Per Abstract: {labels_wt["Sentences_Per Doc"].median()}')
print()

"""> * From the simple metrics above, it can be inferred that there is, for sure, an extreme outlier of 118 sentences.
> * A boxplot of the distribution will give a much better image of it.
"""

import seaborn as sns
sns.boxplot(labels_wt["Sentences_Per Doc"])

"""> * As it can be inferred from the boxplot above, there are some extreme outliers while the vast majority of the distribution is restricted in a range about 10-18.
> * So, let's explore some thresholds that will provide a more solid distribution of the number of sentences per abstract, but without losing much information.
"""

len(labels_wt.doc_id.unique())

print(f'Number of Abstracts with more than 20 sentences: {len(labels_wt.doc_id[labels_wt["Sentences_Per Doc"] >20].unique())}')
print(f'Percentage of Abstracts with more than 20 sentences: {round((len(labels_wt.doc_id[labels_wt["Sentences_Per Doc"] >20].unique()) / \
                                                                     len(labels_wt.doc_id.unique())) *100, 2)} %')

print("")

print(f'Number of Abstracts with more than 25 sentences: {len(labels_wt.doc_id[labels_wt["Sentences_Per Doc"] >25].unique())}')
print(f'Percentage of Abstracts with more than 25 sentences: {round((len(labels_wt.doc_id[labels_wt["Sentences_Per Doc"] >25].unique()) / \
                                                                     len(labels_wt.doc_id.unique())) *100, 2)} %')

print("")

print(f'Number of Abstracts with more than 30 sentences: {len(labels_wt.doc_id[labels_wt["Sentences_Per Doc"] >30].unique())}')
print(f'Percentage of Abstracts with more than 30 sentences: {round((len(labels_wt.doc_id[labels_wt["Sentences_Per Doc"] >30].unique())/ \
                                                                  len(labels_wt.doc_id.unique())) *100, 2)} %')

"""> * Our Choise based on the boxplot and the findings above is to keep a **threshold of 25 sentences** per abstract.
> * This is because on the one hand we do not lose much information with dropping documents above that threshold **(1.08 %)**, but on the other hand we do not obtain much more information with a higher threshold of 30 (1.08% - 0.81% = **0.27%**).
> * So a threshold of 25 sentences seems reasonable based on our data.

Now, it's time to drop the documents above that threshold and see the distribution  of our data again.
"""

#@title Remove the Documents above Threshold and see the value counts of each number of sentences

sentences_wt_final = sentences_wt[sentences_wt['Sentences_Per Doc'] <= 25] # & sentences_wt['Sentences_Per Doc'] > 3
(sentences_wt_final['Sentences_Per Doc'].value_counts()/ \
 sentences_wt_final['Sentences_Per Doc'].value_counts().index)

sentences_wt_final[sentences_wt_final['Sentences_Per Doc'] > 25]

"""The same for Labels"""

labels_wt_final = labels_wt[labels_wt['Sentences_Per Doc'] <= 25] # labels_wt['Sentences_Per Doc'] > 3
(labels_wt_final['Sentences_Per Doc'].value_counts()/ \
 labels_wt_final['Sentences_Per Doc'].value_counts().index)

labels_wt_final[labels_wt_final['Sentences_Per Doc'] > 25]

sns.boxplot(labels_wt_final['Sentences_Per Doc'])

"""## Split Dataset into Train/Test/Validation Sets

> * First of all, for data consistency reason, the split will be made into the doc_ids and after that the corresponding sentences and labels will be introduced in Train, Validation and Test sets.
> * So, we are going to split the documents as a whole and after that, labels and sentences will be introduced, using the doc_ids.
> * In our models the X will be the sentences dataset, while the Y will be the one with the corresponding labels.
>* The datasets that were produced after the data cleansing procedures will, firstly, be split into:

>> 1.   **Train-Validation Set** (85% of the Dataset)
>> 2.   **Test Set** (15% of the dataset)

> * We are going to use only the **"sentence"** column of the sentences dataset and only the **"label"** column of the labels dataset.  


"""

#@title Take the unique Document IDss
sentences_doc_ids = pd.DataFrame(sentences_wt_final.doc_id.unique())
labels_doc_ids = pd.DataFrame(labels_wt_final.doc_id.unique())

from sklearn.model_selection import train_test_split
sentences_train_val_doc_id, sentences_test_doc_id, labels_train_val_doc_id, labels_test_doc_id = train_test_split(sentences_doc_ids, labels_doc_ids,
                                                                                                 test_size=0.15, random_state=42)

"""> * Further split of the **Train-Validation Set** will be made, dividing it into :
>>1.   **Train Set** (80% of the Train-Validation Set)
>>2.   **Validation Set** (20% of the Train-Validation Set)


"""

sentences_train_doc_id, sentences_val_doc_id, labels_train_doc_id, labels_val_doc_id = train_test_split(sentences_train_val_doc_id, labels_train_val_doc_id,
                                                                                                        test_size=0.2, random_state=42)

print(f'Sentences Train-Validation Set Number of Documents: {sentences_train_val_doc_id.shape[0]}')
print(f'Labels Train-Validation Set Number of Documents: {labels_train_val_doc_id.shape[0]}')
print()

print(f'Sentences Train Set Number of Documents: {sentences_train_doc_id.shape[0]}')
print(f'Labels Train Set Number of Documents: {labels_train_doc_id.shape[0]}')
print()

print(f'Sentences Validation Set Number of Documents: {sentences_val_doc_id.shape[0]}')
print(f'Labels Validation Set Number of Documents: {labels_val_doc_id.shape[0]}')

print()
print(f'Sentences Test Set Number of Documents: {sentences_test_doc_id.shape[0]}')
print(f'Labels Test Set Number of Documents: {labels_test_doc_id.shape[0]}')

"""> * Now we are going to split the sentences and labels datasets into the corresponding Sets, based on the **doc_ids** that are in each set.
> * Only for the Training set, two versions of the final Set will be constructed. 
>> 1. The one will contain all the variables as it was constructed before and will be used for **Exploratory Data Analysis**.
>> 2. The other one will have only the **sentence** as described before for tha training of the model. 

>* Exporatory Data Analysis will not be performed for **Test** and **Validation** Sets as they should be the unknown sets for our model.
"""

#@title Sentences Train EDA
sentences_train_EDA = sentences_wt_final.merge(sentences_train_doc_id, how='inner', left_on= 'doc_id', right_on= 0)
sentences_train_EDA = sentences_train_EDA.drop(0,axis='columns')
sentences_train_EDA.head()

#@title Labels Train EDA
labels_train_EDA = labels_wt_final.merge(labels_train_doc_id, how='inner', left_on= 'doc_id', right_on= 0)
labels_train_EDA = labels_train_EDA.drop(0,axis='columns')
labels_train_EDA

# Sentences Train
sentences_train = pd.merge(sentences_wt_final[['doc_id', 'sentence']], sentences_train_doc_id,left_on='doc_id', right_on= 0, how='inner')
sentences_train = sentences_train.drop(['doc_id',0],axis='columns')
sentences_train

# Labels Train
labels_train = pd.merge(labels_wt_final[['doc_id', 'label']], labels_train_doc_id,left_on='doc_id', right_on= 0, how='inner')
labels_train = labels_train.drop(['doc_id',0],axis='columns')
labels_train

# Sentences Test
sentences_test = pd.merge(sentences_wt_final[['doc_id', 'sentence']], sentences_test_doc_id,left_on='doc_id', right_on= 0, how='inner')
sentences_test = sentences_test.drop(['doc_id',0],axis='columns')
sentences_test

# Labels Test
labels_test = pd.merge(labels_wt_final[['doc_id', 'label']], labels_test_doc_id,left_on='doc_id', right_on= 0, how='inner')
labels_test = labels_test.drop(['doc_id',0],axis='columns')
labels_test

# Sentences Validation
sentences_val = pd.merge(sentences_wt_final[['doc_id', 'sentence']], sentences_val_doc_id,left_on='doc_id', right_on= 0, how='inner')
sentences_val = sentences_val.drop(['doc_id',0],axis='columns')
sentences_val

# Labels Val
labels_val = pd.merge(labels_wt_final[['doc_id', 'label']], labels_val_doc_id,left_on='doc_id', right_on= 0, how='inner')
labels_val = labels_val.drop(['doc_id',0],axis='columns')
labels_val

print(f'Sentences Train Shape: {sentences_train.shape}')
print(f'Labels Train Shape: {labels_train.shape}')
print()

print(f'Sentences Test Shape: {sentences_test.shape}')
print(f'Labels Test Shape: {labels_test.shape}')
print()

print(f'Sentences Validation Shape: {sentences_val.shape}')
print(f'Labels Validation Shape: {labels_val.shape}')
print()

print(f'Sum of Sentences of all Sets equal to initial sum before split?: {sentences_train.shape[0] + sentences_test.shape[0] + sentences_val.shape[0] == \
                                                                          sentences_wt_final.shape[0]}')
print(f'Sum of Labels of all Sets equal to initial sum before split?: {labels_train.shape[0] + labels_test.shape[0] + labels_val.shape[0] == \
                                                                          labels_wt_final.shape[0]}')

"""## Exploratory Data Analysis on the Training Set

We are going to merge Labels and Sentences in a common Dataset that will be used for EDA.
"""

sentences_train_EDA

labels_train_EDA

train_EDA = pd.merge(labels_train_EDA['label'], sentences_train_EDA, left_index=True, right_index=True)
train_EDA

"""**Let's Explore the Labels'  Statistics**

"""

counts = train_EDA.label.value_counts()
counts

import seaborn as sns
sns.barplot(x = train_EDA.label.value_counts().index,
            y = train_EDA.label.value_counts())

#Percentages of labels
percentages = round((train_EDA.label.value_counts())/ (train_EDA.label.count()) * 100, 2)
percentages

sns.barplot(x = percentages.index, y = percentages )

"""> We are going to use 3 new variables counting the number of label in each abstract. """



train_EDA['claims'] = 0
train_EDA['evidences'] = 0
train_EDA['neithers'] = 0
len_EDA = len(train_EDA)

for i in  range(0,(len_EDA)):
  if (train_EDA['label'][i] == 'CLAIM'):
    train_EDA['claims'][i] = train_EDA['claims'][i] + 1 
  elif (train_EDA['label'][i] == 'EVIDENCE'):
    train_EDA['evidences'][i] = train_EDA['evidences'][i] + 1
  else:
    train_EDA['neithers'][i] = train_EDA['neithers'][i] + 1 

train_EDA

sum_claims = 0
sum_evidences = 0
sum_neithers = 0
sent = 0

i = 0

while (i < len_EDA):

  sent = train_EDA['Sentences_Per Doc'][i] - 1 # because indexing starts from 0

  sum_claims = 0 
  sum_claims = sum(train_EDA.loc[i : (i + sent), 'claims'])
  train_EDA.loc[i : (i + sent), 'claims'] = sum_claims
  
  sum_evidences = 0 
  sum_evidences = sum(train_EDA.loc[i : (i + sent), 'evidences'])
  train_EDA.loc[i: (i + sent), 'evidences'] = sum_evidences
  
  sum_neithers = 0
  sum_neithers = sum(train_EDA.loc[i : (i + sent), 'neithers'])
  train_EDA.loc[i : (i + sent), 'neithers'] = sum_neithers
  
  sent = 0
  i = i + (train_EDA['Sentences_Per Doc'][i]) 

train_EDA

print(f'Minimum Number of Claims Per Abstract: {train_EDA.claims.min()}')
print(f'Maximum Number of Claims Per Abstract: {train_EDA.claims.max()}')
print(f'Average Number of Claims Per Abstract: {train_EDA.claims.mean()}')
print(f'Median Number of Claims Per Abstract: {train_EDA.claims.median()}')
print()

sns.boxplot(train_EDA.claims)

print(f'Minimum Number of Evidences Per Abstract: {train_EDA.evidences.min()}')
print(f'Maximum Number of Evidences Per Abstract: {train_EDA.evidences.max()}')
print(f'Average Number of Evidences Per Abstract: {train_EDA.evidences.mean()}')
print(f'Median Number of Evidences Per Abstract: {train_EDA.evidences.median()}')
print()

sns.boxplot(train_EDA.evidences)

print(f'Minimum Number of Nones Per Abstract: {train_EDA.neithers.min()}')
print(f'Maximum Number of Nones Per Abstract: {train_EDA.neithers.max()}')
print(f'Average Number of Nones Per Abstract: {train_EDA.neithers.mean()}')
print(f'Median Number of Nones Per Abstract: {train_EDA.neithers.median()}')
print()

sns.boxplot(train_EDA.neithers)

"""A first logical insight from the statistics for the baseline intuitive model can be that we want all the abstracts to have:\
> 1. One up to 2 Claims
> 2. One up to 4 Evidences
> 3. Leave the rest sentences as neither.

Lets's see what is going on in the extreme instances of 1,and 2 sentences per abstract.
"""

#@title One Sentence
train_EDA.label[(train_EDA['Sentences_Per Doc'] == 1)].value_counts()

#@title Two Sentences
train_EDA.label[(train_EDA['Sentences_Per Doc'] == 2)].value_counts()

"""* An Insight from above that can be used for the baseline intuitive classification is that in abstracts with 1 and 2 sentences, evidence is very sparse to appear.
* So the lexicons can help further to decide between CLAIM and NEITHER.

* **Below, we can also see the distribution of each label in each In-Document Sentence ID**
"""

import matplotlib.pylab as plt

plt.plot(train_EDA[(train_EDA['label'] == 'CLAIM') ].In_Doc_Sentence_ID.value_counts().sort_index().reset_index()['index'],
         train_EDA[(train_EDA['label'] == 'CLAIM') ].In_Doc_Sentence_ID.value_counts().sort_index(), label = 'CLAIM')

plt.plot(train_EDA[train_EDA['label'] == 'EVIDENCE'].In_Doc_Sentence_ID.value_counts().sort_index().reset_index()['index'],
         train_EDA[train_EDA['label'] == 'EVIDENCE'].In_Doc_Sentence_ID.value_counts().sort_index(), label = 'EVIDENCE')

plt.plot(train_EDA[train_EDA['label'] == 'NEITHER'].In_Doc_Sentence_ID.value_counts().sort_index().reset_index()['index'],
         train_EDA[train_EDA['label'] == 'NEITHER'].In_Doc_Sentence_ID.value_counts().sort_index(), label = 'NEITHER')

plt.xlabel('In-Document Sentence ID')
plt.ylabel('Counts')
plt.title("Distribution of Labels in Each In-Document Sentence ID")

plt.legend()

plt.show()

"""> * A new Variable will be introduced, which will be a flag taking values:
>>  a. **1** if a sentence is the last one of the abstract.\
>>  b. **0** if it is not.



"""

train_EDA['Is_Last'] = 0
len_EDA = len(train_EDA) 
for i in  range(0, len_EDA):
  if train_EDA['In_Doc_Sentence_ID'][i] == (train_EDA['Sentences_Per Doc'][i]):
    train_EDA['Is_Last'][i] = 1   
train_EDA

"""**Distribution of Labels in the Last Sentence**"""

sns.barplot(x = train_EDA.label[train_EDA['Is_Last'] == 1].value_counts().index,
            y = train_EDA.label[train_EDA['Is_Last'] == 1].value_counts())

train_EDA.label[train_EDA['Is_Last'] == 1].value_counts()

print(f"The {round((train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'CLAIM')].count() /\
train_EDA.label[(train_EDA['Is_Last'] == 1)].count()) * 100, 2)} % of the Last Sentences in each Document are CLAIMS")

print(f"The {round((train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'CLAIM')].count() /\
train_EDA.label[(train_EDA['label'] == 'CLAIM')].count()) * 100, 2)} % of the CLAIMS are in the Last Sentence")
print()

print(f"The {round((train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'EVIDENCE')].count() /\
train_EDA.label[(train_EDA['Is_Last'] == 1)].count()) * 100, 2)} % of the Last Sentences in each Document are EVIDENCES")

print(f"The {round((train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'EVIDENCE')].count() /\
train_EDA.label[(train_EDA['label'] == 'EVIDENCE')].count()) * 100, 2)} % of the EVIDENCES are in the Last Sentence")
print()

print(f"The {round((train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'NEITHER')].count() /\
train_EDA.label[(train_EDA['Is_Last'] == 1)].count()) * 100, 2)} % of the Last Sentences in each Document are NEITHERS")

print(f"The {round((train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'NEITHER')].count() /\
train_EDA.label[(train_EDA['label'] == 'NEITHER')].count()) * 100, 2)} % of the NEITHERS are in the Last Sentence")

"""* **So there is a pretty high probability for the last sentence of each document to be a CLAIM.**

But let's explore it further
"""

plt.plot(train_EDA[(train_EDA['label'] == 'CLAIM') & (train_EDA['Is_Last'] == 1)].In_Doc_Sentence_ID.value_counts().sort_index().reset_index()['index'],
         train_EDA[(train_EDA['label'] == 'CLAIM') & (train_EDA['Is_Last'] == 1)].In_Doc_Sentence_ID.value_counts().sort_index(), label = 'CLAIM')

plt.plot(train_EDA[(train_EDA['label'] == 'EVIDENCE') & (train_EDA['Is_Last'] == 1)].In_Doc_Sentence_ID.value_counts().sort_index().reset_index()['index'],
         train_EDA[(train_EDA['label'] == 'EVIDENCE') & (train_EDA['Is_Last'] == 1)].In_Doc_Sentence_ID.value_counts().sort_index(), label = 'EVIDENCE')

plt.plot(train_EDA[(train_EDA['label'] == 'NEITHER') & (train_EDA['Is_Last'] == 1)].In_Doc_Sentence_ID.value_counts().sort_index().reset_index()['index'],
         train_EDA[(train_EDA['label'] == 'NEITHER') & (train_EDA['Is_Last'] == 1)].In_Doc_Sentence_ID.value_counts().sort_index(), label = 'NEITHER')

plt.xlabel('In-Document Sentence ID')
plt.ylabel('Counts')
plt.title('Distribution of Labels in Each In-Document Sentence ID being also the Last Sentence of tha Abstract')

plt.legend()

plt.show()

print(f"The {round((train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'CLAIM') &\
           ((train_EDA['Sentences_Per Doc'] >= 5) & (train_EDA['Sentences_Per Doc'] <= 16))].count() /\
train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'CLAIM')].count()) * 100, 2) } % of Claims in the Last Sentence are in a range of 5th t 15th sentence of an abstract.")

train_EDA.label[ (train_EDA['Is_Last'] == 1) & (train_EDA['label'] == 'NEITHER') &\
           ((train_EDA['Sentences_Per Doc'] > 16 ))].count()

"""> * After all, a rule for the intuitive baseline model can be that:\
 **a) If a sentence is the last one of an Abstract and it is in the range [5,15] as for the In-Document Sentence ID, it is labeled as "CLAIM"**.\
 **b) If a sentence is the last one of an Abstract and it is in the range [1,5) as for the In-Document Sentence ID, it is labeled as "EVIDENCE"**.\
 **c)If a sentence is the last one of an Abstract and it is in the range (15,25] as for the In-Document Sentence ID, it is labeled as "NEITHER"**
* However, this rule will be further developed, by implementing some key-words which will be described in a while.
"""



"""A new metric like the previous one for the last sentence will be constructed for the the pre-last one and for the one about 2 sentences before the last."""

train_EDA['Is_Pre_Last'] = 0
for i in  range(0,len_EDA - 2):  #-2 because indexing starts from 0 and the last sentence is also last sentence in the abstract it belongs to.
  if ((train_EDA['Sentences_Per Doc'][i] > 1) & (train_EDA['Is_Last'][i + 1] == 1)): # exclude abstracts with one sentence
    train_EDA['Is_Pre_Last'][i] = 1 
train_EDA[train_EDA['Is_Pre_Last'] == 1]

train_EDA['Is_2_Before_Last'] = 0
for i in  range(0,len_EDA - 3):  #-3 because indexing starts from 0 and the last sentence is also last sentence in the abstract it belongs to and the pre-last also.
  if ((train_EDA['Sentences_Per Doc'][i] > 2) & (train_EDA['Is_Pre_Last'][i + 1] == 1)):  # exclude abstracts with 2 sentences
    train_EDA['Is_2_Before_Last'][i] = 1 
train_EDA[train_EDA['Is_2_Before_Last'] == 1]

train_EDA.label[train_EDA['Is_Pre_Last'] == 1 ].value_counts()

"""Continue....

### **Build Lexicons of the most common words for each label**

> * Firstly, we are going to define a counter_vectorizer which will transform the sentences to binary vectors.
> * The stopwords of the English Language will be excluded.
> * We set lowercase parameter to True so a transformation of the sentences to lowercase is needed.
> * The length of the vectors will be 20,000 which means 20,000 different words. That size is far from enough for the size of our data.
> * This procedure will be followed for all the 3 labels seperately, in order to build 3 characteristic lexicons.
"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')


counter_vectorizer = CountVectorizer(
    encoding='utf-8',
    strip_accents='unicode',
    lowercase=True,
    stop_words=stopwords.words('english'),
    ngram_range=(1, 1), # unigrams
    max_features= 40000,
    binary=False # binary output or full counts. 
)
counter_vectorizer

#Transformation to lowercase for Claims
just_sentence_claims = train_EDA.sentence[train_EDA['label'] == 'CLAIM'].str.lower().str.replace('.', ' ')
just_sentence_claims

#Transformation to lowercase for Evidences
just_sentence_evidences = train_EDA.sentence[train_EDA['label'] == 'EVIDENCE'].str.lower().str.replace('.', ' ')
just_sentence_evidences

"""We will also define a function in order to keep numbers out of our lexicon, using regular expressions."""

#Transformation to lowercase for Neithers
just_sentence_neithers = train_EDA.sentence[train_EDA['label'] == 'NEITHER'].str.lower().str.replace('.', ' ')
just_sentence_neithers

import re

def remove_numbers(text):
    text = re.sub(r'\d+', '', text)
    return text

for i in just_sentence_claims.index:
  just_sentence_claims[i] = remove_numbers(just_sentence_claims[i]) 

for i in just_sentence_evidences.index:
  just_sentence_evidences[i] = remove_numbers(just_sentence_evidences[i])
  
for i in just_sentence_neithers.index:
  just_sentence_neithers[i] = remove_numbers(just_sentence_neithers[i])

"""> * Before fitting the Counter Vectorizer, let's have a Visualization of each words that the sentences of each label are consisted of.
> * A WordCloud plot would be ideal to have the "big image"
"""

#Join all the words of the sentences that were transformed to lowercase
corpora_claims = " ".join(just_sentence_claims)
corpora_evidences = " ".join(just_sentence_evidences)
corpora_neithers = " ".join(just_sentence_neithers)

#Claims
import matplotlib.pyplot as plt
from wordcloud import WordCloud
wordcloud = WordCloud(max_font_size=1000, max_words=100, background_color="white").generate(corpora_claims)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#Evidences
import matplotlib.pyplot as plt
from wordcloud import WordCloud
wordcloud = WordCloud(max_font_size=1000, max_words=100, background_color="white").generate(corpora_evidences)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#Neithers
import matplotlib.pyplot as plt
from wordcloud import WordCloud
wordcloud = WordCloud(max_font_size=1000, max_words=100, background_color="white").generate(corpora_neithers)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

# Fit Count Vectorizer for Claims
claims_enc = counter_vectorizer.fit_transform(just_sentence_claims)
claims_enc[0].todense()

words_claims = pd.DataFrame.from_dict(counter_vectorizer.vocabulary_, orient = 'index')

# Fit Count Vectorizer for Evidences
evidences_enc = counter_vectorizer.fit_transform(just_sentence_evidences)
evidences_enc[0].todense()

words_evidences = pd.DataFrame.from_dict(counter_vectorizer.vocabulary_, orient = 'index')

# Fit Count Vectorizer for Evidences
neithers_enc = counter_vectorizer.fit_transform(just_sentence_neithers)
neithers_enc[0].todense()

words_neithers = pd.DataFrame.from_dict(counter_vectorizer.vocabulary_, orient = 'index')

"""> Below we can see the word that corresponds to each position of the vectors."""

counter_vectorizer.vocabulary_['conclusion']

"""> Now it's time to create the lexicons for the top words of each label."""

words_claims.head(2)

#claims

words_claims['words_claims'] = words_claims.index
words_claims['index'] = words_claims[0]
words_claims = words_claims[['words_claims', 'index']]
words_claims = words_claims.sort_values(by='index').reset_index(drop = True)
words_claims

#evidences

words_evidences['words_evidences'] = words_evidences.index
words_evidences['index'] = words_evidences[0]
words_evidences = words_evidences[['words_evidences', 'index']]
words_evidences = words_evidences.sort_values(by='index').reset_index(drop = True)
words_evidences

#neithers

words_neithers['words_neithers'] = words_neithers.index
words_neithers['index'] = words_neithers[0]
words_neithers = words_neithers[['words_neithers', 'index']]
words_neithers = words_neithers.sort_values(by='index').reset_index(drop = True)
words_neithers

# Create a Dataframe for the counts of each label's words

counts_claims = pd.DataFrame(claims_enc.sum(axis=0)).transpose()
counts_claims['index'] = counts_claims.index
counts_claims

counts_evidences = pd.DataFrame(evidences_enc.sum(axis=0)).transpose()
counts_evidences['index'] = counts_evidences.index
counts_evidences

counts_neithers = pd.DataFrame(neithers_enc.sum(axis=0)).transpose()
counts_neithers['index'] = counts_neithers.index
counts_neithers

words_claims.columns

#Claims Lexicon

claims_lexicon = words_claims.merge(counts_claims, how = 'inner', left_on = 'index', right_on='index')
claims_lexicon['counts'] = claims_lexicon[[0]]
claims_lexicon = claims_lexicon[['words_claims', 'index', 'counts']]
claims_lexicon = claims_lexicon.sort_values(by= 'counts' , ascending=False)
claims_lexicon

#Evidences Lexicon

evidences_lexicon = words_evidences.merge(counts_evidences, how = 'inner', left_on = 'index', right_on='index')
evidences_lexicon['counts'] = evidences_lexicon[[0]]
evidences_lexicon = evidences_lexicon[['words_evidences', 'index', 'counts']]
evidences_lexicon = evidences_lexicon.sort_values(by= 'counts' , ascending=False)
evidences_lexicon

#Neithers Lexicon

neithers_lexicon = words_neithers.merge(counts_neithers, how = 'inner', left_on = 'index', right_on='index')
neithers_lexicon['counts'] = neithers_lexicon[[0]]
neithers_lexicon = neithers_lexicon[['words_neithers', 'index', 'counts']]
neithers_lexicon = neithers_lexicon.sort_values(by= 'counts' , ascending=False)
neithers_lexicon

top_words_claims = claims_lexicon[0:100]
top_words_evidences = evidences_lexicon[0:100]
top_words_neithers = neithers_lexicon[0:100]

"""New Metric How many top words in each sentence"""

train_EDA['claims_lexicon_counts'] = 0
train_EDA['evidences_lexicon_counts'] = 0
train_EDA['neithers_lexicon_counts'] = 0

import  tqdm

for i in tqdm.tqdm(range(0,train_EDA.doc_id.count())):
  for w in top_words_claims.words_claims:
      if (train_EDA.sentence[i].find(w) != -1):
        train_EDA.claims_lexicon_counts[i] = train_EDA.claims_lexicon_counts[i] + 1

  for w in top_words_evidences.words_evidences:
      if (train_EDA.sentence[i].find(w) != -1):
        train_EDA.evidences_lexicon_counts[i] = train_EDA.evidences_lexicon_counts[i] + 1

  for w in top_words_neithers.words_neithers:
      if (train_EDA.sentence[i].find(w) != -1):
        train_EDA.neithers_lexicon_counts[i] = train_EDA.neithers_lexicon_counts[i] + 1  

train_EDA.head(10)

train_EDA[['label', 'claims_lexicon_counts', 'evidences_lexicon_counts', 'neithers_lexicon_counts']].head(10)



"""more in lexicons

# Classification based on the Baseline Intuitive model

forloop with conditions
...
"""

import copy
train_EDA_classif = copy.copy(train_EDA)

train_EDA_classif['classif_output'] = 'NEITHER'
len_EDA_classif = len(train_EDA_classif)

for i in tqdm.tqdm(range(0,(len_EDA_classif))):
  if ((train_EDA_classif['Is_Last'][i] == 1) & ((train_EDA_classif['Sentences_Per Doc'][i] >= 5) & (train_EDA_classif['Sentences_Per Doc'][i] <= 16))):
    
    train_EDA_classif['classif_output'][i]= 'CLAIM'

for i in tqdm.tqdm(range(0,(len_EDA_classif))):
  if((train_EDA_classif['claims_lexicon_counts'][i] > (train_EDA_classif['evidences_lexicon_counts'][i]))  & \
     (train_EDA_classif['claims_lexicon_counts'][i] > (train_EDA_classif['neithers_lexicon_counts'][i]+1)) & \
     (train_EDA_classif['claims_lexicon_counts'][i] >  1) &\
     ((train_EDA_classif['Is_Pre_Last'][i] == 1) | (train_EDA_classif['Is_2_Before_Last'][i] == 1)) &\
     (train_EDA_classif['classif_output'][i] == 'NEITHER')):
    
    train_EDA_classif['classif_output'][i]= 'CLAIM'

for i in tqdm.tqdm(range(0,(len_EDA_classif))):
  if ((train_EDA_classif['evidences_lexicon_counts'][i] > (train_EDA_classif['claims_lexicon_counts'][i] ))  & \
    (train_EDA_classif['evidences_lexicon_counts'][i] > (train_EDA_classif['neithers_lexicon_counts'][i])) & \
    (train_EDA_classif['evidences_lexicon_counts'][i] >  3) & (train_EDA_classif['classif_output'][i] != 'CLAIM')):
    
    train_EDA_classif['classif_output'][i]= 'EVIDENCE'

"""Some checks and notes that led to our rules of classification"""

train_EDA_classif.classif_output.value_counts()

train_EDA_classif['evidences_lexicon_counts'].value_counts()

train_EDA_classif['claims_lexicon_counts'].value_counts()

train_EDA_classif.label[(train_EDA_classif['evidences_lexicon_counts'] >= 1) ].value_counts()

train_EDA_classif.label[(train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['claims_lexicon_counts'] +1))  & \
                        (train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['neithers_lexicon_counts'] +1)) & \
                        (train_EDA_classif['evidences_lexicon_counts'] >  5) & \
                        (train_EDA_classif['classif_output'] != 'CLAIM')].value_counts()

train_EDA_classif.label[(train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['claims_lexicon_counts'] ))  & \
                        (train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['neithers_lexicon_counts'])) & \
                        (train_EDA_classif['evidences_lexicon_counts'] >  3) & \
                        (train_EDA_classif['classif_output'] != 'CLAIM')].value_counts()

train_EDA_classif.label[(train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['claims_lexicon_counts'] ))  & \
                        (train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['neithers_lexicon_counts'])) & \
                        (train_EDA_classif['evidences_lexicon_counts'] >  4) ].value_counts()

train_EDA_classif.label[(train_EDA_classif['claims_lexicon_counts'] > (train_EDA_classif['evidences_lexicon_counts']))  & \
                        (train_EDA_classif['claims_lexicon_counts'] > (train_EDA_classif['neithers_lexicon_counts']+1)) & \
                        (train_EDA_classif['claims_lexicon_counts'] >  1) &\
                        ((train_EDA_classif['Is_Pre_Last'] == 1) | (train_EDA_classif['Is_2_Before_Last'] == 1)) &\
                        (train_EDA_classif['classif_output'] == 'NEITHER')].value_counts()

train_EDA_classif.label[(train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['claims_lexicon_counts']))  & \
                        (train_EDA_classif['evidences_lexicon_counts'] > (train_EDA_classif['neithers_lexicon_counts'] +1)) & \
                        #(train_EDA_classif['claims_lexicon_counts'] >  ) &\
                        ((train_EDA_classif['Is_Pre_Last'] == 0) | (train_EDA_classif['Is_2_Before_Last'] == 0)) &\
                        (train_EDA_classif['classif_output'] == 'NEITHER')].value_counts()

train_EDA_classif.label[(train_EDA_classif['claims_lexicon_counts'] > (train_EDA_classif['evidences_lexicon_counts'] +1))  & \
                        (train_EDA_classif['claims_lexicon_counts'] > (train_EDA_classif['neithers_lexicon_counts'] +1)) & \
                        (train_EDA_classif['claims_lexicon_counts'] >  2) &\
                        (train_EDA_classif['Is_2_Before_Last'] == 1)].value_counts()

train_EDA_classif.label[#(train_EDA_classif['evidences_lexicon_counts'] >= train_EDA_classif['claims_lexicon_counts'])  & \
                        (train_EDA_classif['evidences_lexicon_counts'] > train_EDA_classif['neithers_lexicon_counts'])].value_counts()

train_EDA_classif.label.value_counts()

train_EDA_classif.classif_output.value_counts()

"""# Transformer Network Approach
## Version 2: Sentence-Pair Classification
### Approach 4: The Title - Each sentence of the abstract successively
#### 1.a) Argument Labeling - Baseline Intuitive model

# Data Preparation for model fitting
"""

train_setA = pd.merge(sentences_train_doc_id, sentences_wt_final[['sentence', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
train_setA = train_setA[['sentence', 'doc_id']]

test_setA = pd.merge(sentences_test_doc_id, sentences_wt_final[['sentence', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
test_setA = test_setA[['sentence', 'doc_id']]

validation_setA = pd.merge(sentences_val_doc_id, sentences_wt_final[['sentence', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
validation_setA = validation_setA[['sentence', 'doc_id']]



train_setB = pd.merge(sentences_train_doc_id, train_EDA_classif[['classif_output', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
train_setB = train_setB[['classif_output', 'doc_id']]

test_setB = pd.merge(sentences_test_doc_id, labels_wt_final[['label', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
test_setB = test_setB[['label', 'doc_id']]

validation_setB = pd.merge(sentences_val_doc_id, labels_wt_final[['label', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
validation_setB = validation_setB[['label', 'doc_id']]



train_set = pd.merge(train_setA, train_setB[['classif_output']] , how = 'inner', left_index = True , right_index=  True)

test_set = pd.merge(test_setA, test_setB[['label']] , how = 'inner', left_index = True , right_index=  True)

validation_set = pd.merge(validation_setA, validation_setB[['label']] , how = 'inner', left_index = True , right_index=  True)

train_set.columns

train_set = train_set.rename(columns = {'classif_output' : 'label'})

train_set.label[train_set['label'] == 'CLAIM'] = 0
train_set.label[train_set['label'] == 'EVIDENCE'] = 1
train_set.label[train_set['label'] == 'NEITHER'] = 2

test_set.label[test_set['label'] == 'CLAIM'] = 0
test_set.label[test_set['label'] == 'EVIDENCE'] = 1
test_set.label[test_set['label'] == 'NEITHER'] = 2

validation_set.label[validation_set['label'] == 'CLAIM'] = 0
validation_set.label[validation_set['label'] == 'EVIDENCE'] = 1
validation_set.label[validation_set['label'] == 'NEITHER'] = 2



"""> Take the title of each Abstract."""

import copy
sentences_copy = copy.copy(sentences)

sentences_copy['title'] = ''
i = 0
while (i < len(sentences_copy)):
  for j in range(i, (i + sentences_copy['Sentences_Per Doc'][i])):
    sentences_copy['title'][j] = sentences_copy['sentence'][i] 
  
  i = i + sentences_copy['Sentences_Per Doc'][i]


sentences_copy = sentences_copy[['doc_id', 'title']]
sentences_copy2 = sentences_copy.drop_duplicates()

sentences_copy2 = sentences_copy2[['doc_id', 'title']]
sentences_copy2.shape

train_set = sentences_copy2.merge(train_set, left_on='doc_id', right_on='doc_id', how='inner')

test_set = sentences_copy2.merge(test_set, left_on='doc_id', right_on='doc_id', how='inner')

validation_set = sentences_copy2.merge(validation_set, left_on='doc_id', right_on='doc_id', how='inner')

train_set.shape[0] == sentences_train.shape[0]

train_set['sentence1'] = train_set['title']
train_set ['sentence2'] = train_set['sentence']
train_set = train_set[['sentence1', 'sentence2', 'label']]

test_set['sentence1'] = test_set['title']
test_set ['sentence2'] = test_set['sentence'] 
test_set = test_set[['sentence1', 'sentence2', 'label']]

validation_set['sentence1'] = validation_set['title']
validation_set ['sentence2'] = validation_set['sentence']
validation_set = validation_set[['sentence1', 'sentence2', 'label']]

import numpy as np

test_set_labels1 = test_set['label']
test_set_labels1 = (np.array(test_set_labels1).astype('int'))



!pip install datasets
import datasets
from datasets import dataset_dict
import pyarrow as pa

train_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(train_set))

test_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(test_set))

validation_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(validation_set))

raw_datasets = {'train': train_set, 'test' : test_set, 'validation' : validation_set}
raw_datasets

raw_datasets['train'][0]

raw_datasets['train'].features

!pip install transformers
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets_train = raw_datasets['train'].map(tokenize_function, batched=True)
tokenized_datasets_test = raw_datasets['test'].map(tokenize_function, batched=True)
tokenized_datasets_validation = raw_datasets['validation'].map(tokenize_function, batched=True)

tokenized_datasets_train

tokenized_datasets_train[2]

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

from transformers import Trainer
import os
os.CUDA_LAUNCH_BLOCKING=1

trainer = Trainer(
    model,
    training_args,
    train_dataset = tokenized_datasets_train,
    eval_dataset = tokenized_datasets_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

predictions1 = trainer.predict(tokenized_datasets_test)
print(predictions1.predictions.shape, predictions1.label_ids.shape)

import numpy as np
preds1 = np.argmax(predictions1.predictions, axis=-1)

preds1

from sklearn.metrics import classification_report

print(classification_report(test_set_labels1, preds1))

from itertools import cycle
from typing import List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import interp

from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.models import Sequential

def print_confusion_matrix(y_true,
                           y_pred,
                           class_names: List[str],
                           figsize: Tuple[int, int] = (10, 7),
                           fontsize: int = 14) -> pd.DataFrame:
    """
    Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heat-map.

    For something more extraordinary check this repo:
    https://github.com/wcipriano/pretty-print-confusion-matrix


    :param class_names:  An ordered list of class names
    :param figsize: A 2-long tuple, the first value determining the horizontal size of the outputted
                    figure, the second determining the vertical size. Defaults to (10,7).
    :param fontsize: Font size for axes labels. Defaults to 14.
    :return: The confusion matrix as a dataset
    """
    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred)

    df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)

    fig = plt.figure(figsize=figsize)

    try:
        heatmap = sns.heatmap(df_cm, annot=True, fmt="d")

    except ValueError:

        raise ValueError("Confusion matrix values must be integers.")

    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(),
                                 rotation=0,
                                 ha='right',
                                 fontsize=fontsize)

    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(),
                                 rotation=45,
                                 ha='right',
                                 fontsize=fontsize)

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

    return df_cm

n_classes = 3
print_confusion_matrix(y_true=test_set_labels1,
                       y_pred=preds1,
                       class_names=range(n_classes))



"""# Transformer Network Approach
## Version 2: Sentence-Pair Classification
### Approach 4: The Title - Each sentence of the abstract successively
#### 1.b) Argument Labeling on the actual labels of the dataset as were given.

##### Data Preparation for model fitting
"""

train_setA = pd.merge(sentences_train_doc_id, sentences_wt_final[['sentence', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
train_setA = train_setA[['sentence', 'doc_id']]

test_setA = pd.merge(sentences_test_doc_id, sentences_wt_final[['sentence', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
test_setA = test_setA[['sentence', 'doc_id']]

validation_setA = pd.merge(sentences_val_doc_id, sentences_wt_final[['sentence', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
validation_setA = validation_setA[['sentence', 'doc_id']]

train_setB = pd.merge(sentences_train_doc_id, labels_wt_final[['label', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
train_setB = train_setB[['label', 'doc_id']]

test_setB = pd.merge(sentences_test_doc_id, labels_wt_final[['label', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
test_setB = test_setB[['label', 'doc_id']]

validation_setB = pd.merge(sentences_val_doc_id, labels_wt_final[['label', 'doc_id']], how = 'inner', left_on = 0, right_on = 'doc_id')
validation_setB = validation_setB[['label', 'doc_id']]

train_set = pd.merge(train_setA, train_setB[['label']] , how = 'inner', left_index = True , right_index=  True)

test_set = pd.merge(test_setA, test_setB[['label']] , how = 'inner', left_index = True , right_index=  True)

validation_set = pd.merge(validation_setA, validation_setB[['label']] , how = 'inner', left_index = True , right_index=  True)

train_set.label[train_set['label'] == 'CLAIM'] = 0
train_set.label[train_set['label'] == 'EVIDENCE'] = 1
train_set.label[train_set['label'] == 'NEITHER'] = 2

test_set.label[test_set['label'] == 'CLAIM'] = 0
test_set.label[test_set['label'] == 'EVIDENCE'] = 1
test_set.label[test_set['label'] == 'NEITHER'] = 2

validation_set.label[validation_set['label'] == 'CLAIM'] = 0
validation_set.label[validation_set['label'] == 'EVIDENCE'] = 1
validation_set.label[validation_set['label'] == 'NEITHER'] = 2

"""> Take the title of each Abstract."""

import copy
sentences_copy = copy.copy(sentences)

sentences_copy['title'] = ''
i = 0
while (i < len(sentences_copy)):
  for j in range(i, (i + sentences_copy['Sentences_Per Doc'][i])):
    sentences_copy['title'][j] = sentences_copy['sentence'][i] 
  
  i = i + sentences_copy['Sentences_Per Doc'][i]


sentences_copy = sentences_copy[['doc_id', 'title']]
sentences_copy2 = sentences_copy.drop_duplicates()

sentences_copy2 = sentences_copy2[['doc_id', 'title']]
sentences_copy2.shape

train_set = sentences_copy2.merge(train_set, left_on='doc_id', right_on='doc_id', how='inner')

test_set = sentences_copy2.merge(test_set, left_on='doc_id', right_on='doc_id', how='inner')

validation_set = sentences_copy2.merge(validation_set, left_on='doc_id', right_on='doc_id', how='inner')

train_set.shape[0] == sentences_train.shape[0]

train_set

train_set['sentence1'] = train_set['title']
train_set ['sentence2'] = train_set['sentence']
train_set = train_set[['sentence1', 'sentence2', 'label']]

test_set['sentence1'] = test_set['title']
test_set ['sentence2'] = test_set['sentence'] 
test_set = test_set[['sentence1', 'sentence2', 'label']]

validation_set['sentence1'] = validation_set['title']
validation_set ['sentence2'] = validation_set['sentence']
validation_set = validation_set[['sentence1', 'sentence2', 'label']]

test_set_labels2 = test_set['label']
test_set_labels2 = (np.array(test_set_labels2)).astype('int')

#!pip install datasets
#import datasets
#from datasets import dataset_dict
#import pyarrow as pa

train_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(train_set))

test_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(test_set))

validation_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(validation_set))

raw_datasets = {'train': train_set, 'test' : test_set, 'validation' : validation_set}
raw_datasets

raw_datasets['train'][0]

raw_datasets['train'].features

#!pip install transformers
#from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets_train = raw_datasets['train'].map(tokenize_function, batched=True)
tokenized_datasets_test = raw_datasets['test'].map(tokenize_function, batched=True)
tokenized_datasets_validation = raw_datasets['validation'].map(tokenize_function, batched=True)

#from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

#from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")

#from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

#from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset = tokenized_datasets_train,
    eval_dataset=tokenized_datasets_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
)



raw_datasets['train'].features

trainer.train()

predictions2 = trainer.predict(tokenized_datasets_test)
print(predictions2.predictions.shape, predictions2.label_ids.shape)

import numpy as np
preds2 = np.argmax(predictions2.predictions, axis=-1)

from sklearn.metrics import classification_report

print(classification_report(test_set_labels2, preds2))

n_classes = 3
print_confusion_matrix(y_true=test_set_labels2,
                       y_pred=preds2,
                       class_names=range(n_classes))

"""# Transformer Network Approach
## Version 2: Sentence-Pair Classification
### Approach 4: The whole Abstract - Each sentence of the abstract successively
#### 2) Structure Labeling

> There was an encdoing issue in 3 abstracts and was resolved by removing them after communication with Mr.Fergadis.
"""

import json
import pandas as pd

def load_corpus(path, label_mapping=None):
    with open(path, encoding='utf-8-sig') as fp:
        corpus = json.load(fp)

    documents, texts, labels = [], [], []
    for abstract in corpus:
        documents.append(abstract)
        texts.append(corpus[abstract]['sentences'])
        if isinstance(label_mapping, dict):
            labels.append(
                [label_mapping[str(l).upper()]
                    for l in corpus[abstract]['labels']])
        else:
            labels.append([str(l).upper() for l in corpus[abstract]['labels']])

    assert len(texts) == len(labels)
    data = pd.DataFrame(
        zip(documents, texts, labels),
        columns=['document', 'sentences', 'labels'])

    return data

data_structure = load_corpus('/content/drive/MyDrive/ML_PROJECT/Data/test_str.json') 
#data_argument1 = load_corpus('/content/drive/MyDrive/ML_PROJECT/Data/dataset_aueb_structure_v2.json')
print(f'Dataset length: {len(data_structure)} abstracts')
data_structure.sample(5)

#@title Split to sentences
sentences_str = data_structure['sentences'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'sentences': 'sentence'})
sentences_str.head()

#@title and the corresponding labels
labels_str = data_structure['labels'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'labels': 'label'})
labels_str.head()

labels_str.shape

data_str = sentences_str.merge(labels_str[['label']], how = 'inner', left_index = True, right_index= True)
data_str.shape

#Sentences_Per_Data_Str
sentences_per_data_str = data_str.doc_id.value_counts()
sentences_per_data_str = sentences_per_data_str.sort_index()

ddd = {'doc_id': sentences_per_data_str.index, 
     'Sentences_Per_Doc': sentences_per_data_str}
Sentences_Per_Data_clust_finalstract = pd.DataFrame(data = ddd)
Sentences_Per_Data_clust_finalstract

data_str = data_str.merge(Sentences_Per_Data_clust_finalstract[['doc_id', 'Sentences_Per_Doc']],
                             how = 'inner',
                            left_on = 'doc_id', right_on = 'doc_id')
data_str

data_str = data_str[(data_str['Sentences_Per_Doc'] <= 26) & (data_str['Sentences_Per_Doc'] > 4)]
data_str

data_str.doc_id.unique().shape

data_str.label.value_counts()

titles_str = data_str[['doc_id', 'sentence']][data_str['label'] == 'NEITHER']
titles_str['title'] = titles_str['sentence']
titles_str = titles_str.drop('sentence', axis = 1)
titles_str

data_str = data_str[data_str['label'] != 'NEITHER']

data_str = data_str.merge(titles_str, how = 'inner', on='doc_id')

data_str['sentence1'] = data_str['title']
data_str['sentence2'] = data_str['sentence']
data_str = data_str[['doc_id','label', 'sentence1', 'sentence2']]
data_str.columns

data_str.label[data_str['label'] == 'RESULT'] = 0
data_str.label[data_str['label'] == 'BACKGROUND'] = 1
data_str.label[data_str['label'] == 'METHOD'] = 2
data_str.label[data_str['label'] == 'CONCLUSION'] = 3
data_str.label[data_str['label'] == 'OBJECTIVE'] = 4

"""> Split to Train/Validation/Test Set.
> This split will also be used for the Citances model and the Clustering task as they come from the same abstract.
> However arguments in the first task were of 2 datasets as we saw. This one and the sample dataset that was initially distributed to us.
"""

docs = pd.DataFrame({'doc_id':titles_str.doc_id}).reset_index(drop = True)

sentences_train_val_doc_id, sentences_test_doc_id, labels_train_val_doc_id, labels_test_doc_id = train_test_split(docs, docs,
                                                                                                 test_size=0.15, random_state=42)

(sentences_train_val_doc_id == labels_train_val_doc_id).value_counts()

sentences_train_doc_id, sentences_val_doc_id, labels_train_doc_id, labels_val_doc_id = train_test_split(sentences_train_val_doc_id, labels_train_val_doc_id,
                                                                                                        test_size=0.2, random_state=42)

print(f'Sentences Train-Validation Set Number of Documents: {sentences_train_val_doc_id.shape[0]}')
print(f'Labels Train-Validation Set Number of Documents: {labels_train_val_doc_id.shape[0]}')
print()

print(f'Sentences Train Set Number of Documents: {sentences_train_doc_id.shape[0]}')
print(f'Labels Train Set Number of Documents: {labels_train_doc_id.shape[0]}')
print()

print(f'Sentences Validation Set Number of Documents: {sentences_val_doc_id.shape[0]}')
print(f'Labels Validation Set Number of Documents: {labels_val_doc_id.shape[0]}')

print()
print(f'Sentences Test Set Number of Documents: {sentences_test_doc_id.shape[0]}')
print(f'Labels Test Set Number of Documents: {labels_test_doc_id.shape[0]}')

sentences_train_doc_id

#Train
structure_train = pd.merge(data_str[['doc_id', 'label', 'sentence1', 'sentence2']], sentences_train_doc_id, on='doc_id', how='inner')
structure_train = structure_train[['label', 'sentence1', 'sentence2']]

structure_test = pd.merge(data_str[['doc_id', 'label', 'sentence1', 'sentence2']], sentences_test_doc_id, on='doc_id', how='inner')
structure_test = structure_test[['label', 'sentence1', 'sentence2']]

structure_val = pd.merge(data_str[['doc_id', 'label', 'sentence1', 'sentence2']], sentences_val_doc_id, on='doc_id', how='inner')
structure_val = structure_val[['label', 'sentence1', 'sentence2']]

test_set_labels3 = structure_test['label']
test_set_labels3 = (np.array(test_set_labels3)).astype('int')

print(f'Structure Train Shape: {structure_train.shape}')
print()

print(f'Structure Test Shape: {structure_test.shape}')
print()

print(f'Structure Validation Shape: {structure_val.shape}')
print()

print(f'Sum of Sentences of all Sets equal to initial sum before split?: {structure_train.shape[0] + structure_test.shape[0] + structure_val.shape[0] == \
                                                                          data_str.shape[0]}')

train_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(structure_train))

test_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(structure_test))

validation_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(structure_val))

raw_datasets = {'train': train_set, 'test' : test_set, 'validation' : validation_set}
raw_datasets

raw_datasets['train'][0]

raw_datasets['train'].features

tokenized_datasets_train = raw_datasets['train'].map(tokenize_function, batched=True)
tokenized_datasets_test = raw_datasets['test'].map(tokenize_function, batched=True)
tokenized_datasets_validation = raw_datasets['validation'].map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments("test-trainer")

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 5)

trainer = Trainer(
    model,
    training_args,
    train_dataset = tokenized_datasets_train,
    eval_dataset=tokenized_datasets_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

predictions3 = trainer.predict(tokenized_datasets_test)
print(predictions3.predictions.shape, predictions3.label_ids.shape)

#import numpy as np
preds3 = np.argmax(predictions3.predictions, axis=-1)

from sklearn.metrics import classification_report

print(classification_report(test_set_labels3, preds3))

n_classes = 5
print_confusion_matrix(y_true=test_set_labels3,
                       y_pred=preds3,
                       class_names=range(n_classes))

n_classes = 5
plot_multi_class_roc_auc_curves(nb_classes=n_classes,
                                y_true=test_set_labels3,
                                y_pred_score=preds3)

"""# Transformer Network Approach
## Version 2: Sentence-Pair Classification
### Approach 4: The whole Abstract - Each sentence of the abstract successively
#### 3) Citances Labeling
"""

data_citations = load_corpus('/content/drive/MyDrive/ML_PROJECT/Data/dataset_aueb_citations_v2.json')
print(f'Dataset length: {len(data_citations)} abstracts')
data_citations.sample(5)

#@title Split to sentences
citances1 = data_citations['sentences'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'sentences': 'sentence'})
citances1.head()

#@title and the corresponding labels
citances_labels1 = data_citations['labels'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'labels': 'label'})
citances_labels1.head(2)

#@title Regroup document using doc_id
doc0_sents1 = citances1.query('doc_id==0')['sentence']
doc0_labels1 = citances_labels1.query('doc_id==0')['label']
for label, sent in zip(doc0_labels1, doc0_sents1):
    print(f'{str(label):5s} {sent}')

citances_labels1.columns

dataset_citances = citances1.merge(citances_labels1[['label']], how= 'inner', left_index = True, right_index = True) 
dataset_citances.shape

dataset_citances[dataset_citances['doc_id'] == 0]

dataset_citances.label.value_counts()

"""Remove Titles having 'NEITHER' Label"""

dataset_citances = dataset_citances[dataset_citances['label'] != 'NEITHER']
dataset_citances = dataset_citances.reset_index(drop=True)
dataset_citances.label.value_counts()

dataset_citances.sentence[0].split('\n')

dataset_citances.sentence[0].split('\n')[1]

for i in range (0, len(dataset_citances)):
  dataset_citances.sentence[i] = dataset_citances.sentence[i].split('\n')[1]

dataset_citances.sentence[0]

"""Split to Train/Validation/Test Set"""

dataset_citances.columns

#Train
citances_train = pd.merge(dataset_citances, sentences_train_doc_id, on='doc_id', how='inner')

citances_test = pd.merge(dataset_citances, sentences_test_doc_id, on='doc_id', how='inner')

citances_val = pd.merge(dataset_citances, sentences_val_doc_id, on='doc_id', how='inner')

print(f'Structure Train Shape: {structure_train.shape}')
print()

print(f'Structure Test Shape: {structure_test.shape}')
print()

print(f'Structure Validation Shape: {structure_val.shape}')
print()

print(f'Sum of Sentences of all Sets equal to initial sum before split?: {structure_train.shape[0] + structure_test.shape[0] + structure_val.shape[0] == \
                                                                          data_str.shape[0]}')

"""Claims and Claims_Evidences
> Claims will be used for the Citances model.\
> However we want both Claims and Claims Evidences for the next task of Clustering.
"""

temp_claims = sentences[['doc_id','sentence']].merge(labels[['label']], how = 'inner', left_index = True, right_index  =True)
claims = temp_claims[temp_claims['label'] == 'CLAIM']
claims_evidences = temp_claims[(temp_claims['label'] == 'CLAIM') | (temp_claims['label'] == 'EVIDENCE') ]

claims = claims.groupby(['doc_id']).sum()
claims['claim'] = claims['sentence']
claims['doc_id'] = claims.index
claims = claims[['doc_id', 'claim']]
claims

claims_evidences = claims_evidences.groupby(['doc_id']).sum()
claims_evidences['claim_evidence'] = claims_evidences['sentence']
claims_evidences['doc_id'] = claims_evidences.index
claims_evidences = claims_evidences[['doc_id', 'claim_evidence']]
claims_evidences

"""Reset the index because the name of the index is 'doc_id' and it can be ambiguous in the merges that will take place afterwards, as we also have a column named 'doc_id'"""

claims = claims.reset_index(drop =True)
claims_evidences = claims_evidences.reset_index(drop=True)

claims.claim[3]



claims_evidences.claim_evidence[3]

citances_train = claims.merge(citances_train, how = 'inner', on = 'doc_id')
citances_train['sentence1'] = citances_train['claim']
citances_train['sentence2'] = citances_train['sentence']

print(f'Citance Train Sentences: {citances_train.shape}')
print()

print(f'Citance Train Documents: {len(citances_train.doc_id.unique())}')
print()

citances_train = citances_train[['label', 'sentence1', 'sentence2']]

citances_test = claims.merge(citances_test, how = 'inner', on = 'doc_id')
citances_test['sentence1'] = citances_test['claim']
citances_test['sentence2'] = citances_test['sentence']

print(f'Citance Test Sentences: {citances_test.shape}')
print()

print(f'Citance Test Documents: {len(citances_test.doc_id.unique())}')
print()

citances_test = citances_test[['label', 'sentence1', 'sentence2']]

citances_val = claims.merge(citances_val, how = 'inner', on = 'doc_id')
citances_val['sentence1'] = citances_val['claim']
citances_val['sentence2'] = citances_val['sentence']

print(f'Citance Validation Sentences: {citances_val.shape}')
print()

print(f'Citance Validation Documents: {len(citances_val.doc_id.unique())}')
print()

citances_val = citances_val[['label', 'sentence1', 'sentence2']]

citances_val.label.value_counts()

citances_train.label[citances_train['label'] == 'IRRELEVANT'] = 0
citances_train.label[citances_train['label'] == 'NEUTRAL'] = 1
citances_train.label[citances_train['label'] == 'POSITIVE'] = 2
citances_train.label[citances_train['label'] == 'NEGATIVE'] = 3

citances_test.label[citances_test['label'] == 'IRRELEVANT'] = 0
citances_test.label[citances_test['label'] == 'NEUTRAL'] = 1
citances_test.label[citances_test['label'] == 'POSITIVE'] = 2
citances_test.label[citances_test['label'] == 'NEGATIVE'] = 3

citances_val.label[citances_val['label'] == 'IRRELEVANT'] = 0
citances_val.label[citances_val['label'] == 'NEUTRAL'] = 1
citances_val.label[citances_val['label'] == 'POSITIVE'] = 2
citances_val.label[citances_val['label'] == 'NEGATIVE'] = 3

citances_train.columns

test_set_labels4 = citances_test['label']
test_set_labels4 = (np.array(test_set_labels4)).astype('int')

train_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(citances_train))

test_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(citances_test))

validation_set = datasets.dataset_dict.Dataset(pa.Table.from_pandas(citances_val))

raw_datasets = {'train': train_set, 'test' : test_set, 'validation' : validation_set}
raw_datasets

raw_datasets['train'][0]

raw_datasets['train'].features

#checkpoint = "bert-base-uncased"
#tokenizer = AutoTokenizer.from_pretrained(checkpoint)

tokenized_datasets_train = raw_datasets['train'].map(tokenize_function, batched=True)
tokenized_datasets_test = raw_datasets['test'].map(tokenize_function, batched=True)
tokenized_datasets_validation = raw_datasets['validation'].map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments("test-trainer")

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=4)

#from transformers import Trainer


trainer = Trainer(
    model,
    training_args,
    train_dataset = tokenized_datasets_train,
    eval_dataset=tokenized_datasets_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

predictions4 = trainer.predict(tokenized_datasets_test)
print(predictions4.predictions.shape, predictions4.label_ids.shape)

#import numpy as np
preds4 = np.argmax(predictions4.predictions, axis=-1)

preds4.shape

test_set_labels4

((np.array(test_set_labels4)).astype('int')).dtype

from sklearn.metrics import classification_report

print(classification_report(test_set_labels4, preds4))

n_classes = 4
print_confusion_matrix(y_true=test_set_labels4,
                       y_pred=preds4,
                       class_names=range(n_classes))

"""K-means Clustering"""

import json
import pandas as pd

import requests


def load_corpus_clustering(path, label_mapping=None):
    with open(path,  encoding='utf-8-sig') as fp:
      corpus = json.load(fp)

    documents, texts, labels, eu_call, project_objective = [], [], [], [], []
    for abstract in corpus:
        documents.append(abstract)
        texts.append(corpus[abstract]['sentences'])
        eu_call.append(corpus[abstract]['eu_call'])
        project_objective.append(corpus[abstract]['project_objective'])
        if isinstance(label_mapping, dict):
            labels.append(
                [label_mapping[str(l).upper()]
                    for l in corpus[abstract]['labels']])
        else:
            labels.append([str(l).upper() for l in corpus[abstract]['labels']])

    assert len(texts) == len(labels)
    data = pd.DataFrame(
        zip(documents, texts, labels, eu_call, project_objective),
        columns=['document', 'sentences', 'labels', 'eu_call', 'project_objective'])

    return data

data_clust = load_corpus_clustering('/content/drive/MyDrive/ML_PROJECT/Data/dataset_aueb_argument_v3.json')
print(f'Dataset length: {len(data_clust)} abstracts')
data_clust.sample(5)

temp = data_clust[['eu_call', 'project_objective']].reset_index()
temp.columns

#@title Split to sentences
data_clust_sentences = data_clust['sentences'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'sentences': 'sentence'})
data_clust_sentences.head(2)

#@title Split to labels
data_clust_labels = data_clust['labels'].explode().reset_index().rename(
    columns={'index': 'doc_id', 'labels': 'label'})
data_clust_labels.head(2)

data_clust2 = data_clust_sentences.merge(data_clust_labels[['label']], how = 'inner', left_index = True, right_index= True)
data_clust2.shape

data_clust_final = data_clust2.merge(temp, how = 'inner', left_on = 'doc_id', right_on = 'index')
data_clust_final.head()

data_clust_final.shape

#Sentences_Per_Data_clust_finalstract
sentences_per_data_clust_finalstract = data_clust_final.doc_id.value_counts()
sentences_per_data_clust_finalstract = sentences_per_data_clust_finalstract.sort_index()

ddd = {'doc_id': sentences_per_data_clust_finalstract.index, 
     'Sentences_Per_Doc': sentences_per_data_clust_finalstract}
Sentences_Per_Data_clust_finalstract = pd.DataFrame(data = ddd)
Sentences_Per_Data_clust_finalstract

data_clust_final = data_clust_final.merge(Sentences_Per_Data_clust_finalstract[['doc_id', 'Sentences_Per_Doc']],
                             how = 'inner',
                            left_on = 'doc_id', right_on = 'doc_id')

data_clust_final['abstract'] = ''
data_clust_final['abstract'][0] = data_clust_final['sentence'][0]
for i in range(1, len(data_clust_final)):
  if (data_clust_final['doc_id'][i] == data_clust_final['doc_id'][i-1]):
    data_clust_final['abstract'][i] = data_clust_final['abstract'][i-1] + data_clust_final['sentence'][i]
  else:
    data_clust_final['abstract'][i] =  data_clust_final['sentence'][i]



i = 0
while (i < len(data_clust_final)):
  for j in range(i, (i + data_clust_final['Sentences_Per_Doc'][i])):
    data_clust_final['abstract'][j] = data_clust_final['abstract'][(i + data_clust_final['Sentences_Per_Doc'][i] - 1)] 
  
  i = i + data_clust_final['Sentences_Per_Doc'][i]

data_clust_final['abstract'][17] == data_clust_final['abstract'][33]

data_clust_final = data_clust_final[(data_clust_final['Sentences_Per_Doc'] <= 26) & (data_clust_final['Sentences_Per_Doc'] > 4)]

data_clust_final.head(1)

data_clust_final = data_clust_final.merge(claims, how = 'inner' , on = 'doc_id')
data_clust_final = data_clust_final.merge(claims_evidences, how = 'inner' , on = 'doc_id')
data_clust_final

data_clust_final.claim_evidence[100]

data_clust_final.claim[100]

data_clust_final

"""Import EU CALLS"""

path = '/content/drive/MyDrive/ML_PROJECT/Data/eu_calls.json'
with open(path) as fp:
      corpus = json.load(fp)
eucalls = pd.DataFrame({'eu_call_id' : list(corpus), 'EU_CALL': list(corpus.values())})
eucalls

data_clust_final = data_clust_final.merge(eucalls, how='inner', left_on='eu_call', right_on='eu_call_id')

cluster_data = data_clust_final[['abstract', 'project_objective', 'EU_CALL', 'claim', 'claim_evidence']].drop_duplicates().reset_index(drop =True)
cluster_data

"""> 1. Abstract Word Embeddings Clustering"""

import os
from glob import glob
from typing import Tuple, List
import pandas as pd
import numpy as np
from tensorflow.python.keras.layers import Embedding, Flatten, Dense
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tqdm import tqdm
from ipywidgets import interact

max_words = 15_000  # We will only consider the 15K most used words in this dataset

# Setting up Keras tokenizer
clust_tokenizer = Tokenizer(num_words=max_words, lower=True, oov_token='<OOV>')

clust_tokenizer

# this is like the .fit() that we call we using Scikit-learn and Count-Vectorizer
clust_tokenizer.fit_on_texts(cluster_data['abstract'] )  # Generate tokens by counting frequency

sequences = clust_tokenizer.texts_to_sequences(cluster_data['abstract'])

# The tokenizers word index is a dictionary that maps each word to a number.
# You can see that words that are frequently used in discussions about
# movies have a lower token number.
word_index = clust_tokenizer.word_index
word_index

cluster_data.claim[0].split()

# To proceed, we now have to make sure that all text sequences we feed into the model
# have the same length.

# We can do this with Keras pad sequences tool.
# It cuts of sequences that are too long and adds zeros to sequences that are too short.

# Make all sequences 200 words long
maxlen = 200

data = pad_sequences(sequences, maxlen=maxlen)

# We have 25K, 100 word sequences now
print('New data shape: {}'.format(data.shape))

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

def load_glove_embeddings(dim: int = 100) -> dict:
    """
    Function that loads glove embeddings. 

    :param dim: The embeddings size (dimensions)
    :return:
    """
    print('Loading word vectors')

    embed_index = dict()  # We create a dictionary of word -> embedding

    fname = 'glove.6B.{}d.txt'.format(dim)

    f = open(fname, encoding="utf8")  # Open file

    # In the dataset, each line represents a new word embedding
    # The line starts with the word and the embedding values follow
    for line in tqdm(f, desc='Loading Embeddings', unit='word'):
        values = line.split()
        # The first value is the word, the rest are the values of the embedding
        word = values[0]
        # Load embedding
        embedding = np.asarray(values[1:], dtype='float32')

        # Add embedding to our embedding dictionary
        embed_index[word] = embedding
    f.close()

    print('Found %s word vectors.' % len(embed_index))

    return embed_index

embedding_dim = 300  # We now use larger embeddings

embeddings_index = load_glove_embeddings(dim=embedding_dim)

def create_embeddings_matrix(emb_index: dict,
                             tokenizer: Tokenizer,
                             emb_dim: int = 100) -> np.ndarray:
    """

    :param emb_index: Embeddings Index
    :param tokenizer: Keras fitted tokenizer.
    :param emb_dim: Embeddings dimension.
    :return: A matrix of shape (nb_words, emb_dim) containing the globe embeddings.
    """
    assert emb_dim in [50, 100, 200, 300]

    # Create a matrix of all embeddings
    # (stacking=concatenating all the vectors)
    all_embs = np.stack(emb_index.values())  # .values() gets the all the arrays from the keys

    # Calculate mean
    emb_mean = all_embs.mean()
    # Calculate standard deviation
    emb_std = all_embs.std()

    print("Embeddings AVG: {} | STD: {}".format(emb_mean, emb_std))

    # We can now create an embedding matrix holding all word vectors.

    word_index = tokenizer.word_index

    # How many words are there actually. Because we may have requested X most common tokens
    # and the total tokens are X/2
    nb_words = min(max_words, len(word_index))

    # Create a random matrix with the same mean and std as the embeddings

    embedding_matrix = np.random.normal(emb_mean,  # mean
                                        emb_std,  # std
                                        (nb_words, emb_dim)) # shape of the matrix

    # The vectors need to be in the same position as their index.
    # Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on
    
    counter = 0
    
    # Loop over all words in the word index
    for word, i in word_index.items():  # .items() return a tuple with (word, word_index)

        # If we are above the amount of words we want to use we do nothing
        if i >= max_words:
            continue

        # Get the embedding vector for the word
        embedding_vector = emb_index.get(word)

        # If there is an embedding vector, put it in the embedding matrix
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            counter += 1
    
    print(f'Found {counter} pre-trained embeddings out of {nb_words}')
    
    return embedding_matrix

embedding_matrix = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

embedding_matrix

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

from yellowbrick.cluster import KElbowVisualizer

# Generate synthetic dataset with 8 random clusters
X, y =  embedding_matrix, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

from yellowbrick.cluster import InterclusterDistance

# Generate synthetic dataset with 12 random clusters
X, y = embedding_matrix, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(11)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

from sklearn.cluster import KMeans
from yellowbrick.cluster import SilhouetteVisualizer

model = KMeans(11)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 2. Abstract - Project Objective Word Embeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] +cluster_data['project_objective']))  # Generate tokens by counting frequency

sequences2 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] +cluster_data['project_objective']))

word_index2 = clust_tokenizer.word_index
word_index2

# Make all sequences 200 words long
maxlen = 200

data2 = pad_sequences(sequences2, maxlen=maxlen)

# We have 25K, 200 word sequences now
print('New data shape: {}'.format(data2.shape))

embedding_matrix2 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix2, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix2, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(8)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(8)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 3. Abstract - EU Call Word Embeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] +cluster_data['EU_CALL']))  # Generate tokens by counting frequency
sequences3 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] +cluster_data['EU_CALL']))
word_index = clust_tokenizer.word_index

maxlen = 200

data3 = pad_sequences(sequences3, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data3.shape))

embedding_matrix3 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix3, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix3, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(9)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(9)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 4. Abstract - EU Call - Project Objective Word Embeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] + cluster_data['EU_CALL'] + cluster_data['project_objective']))  # Generate tokens by counting frequency
sequences4 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] + cluster_data['EU_CALL']  +cluster_data['project_objective']))
word_index = clust_tokenizer.word_index

maxlen = 200

data4 = pad_sequences(sequences4, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data4.shape))

embedding_matrix4 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix4, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix4, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(10)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(10)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 5. Abstract - Project Objective - Claim"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] +cluster_data['project_objective'] + cluster_data['claim']))  # Generate tokens by counting frequency
sequences5 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] +cluster_data['project_objective']+ cluster_data['claim']))
word_index = clust_tokenizer.word_index

maxlen = 200

data5 = pad_sequences(sequences5, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data5.shape))

embedding_matrix5 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix5, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix5, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(9)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(9)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 6. Abstract - Project Objective - Claim/Evidence Word Embeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] +cluster_data['project_objective'] + cluster_data['claim_evidence']))  # Generate tokens by counting frequency
sequences6 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] +cluster_data['project_objective'] + cluster_data['claim_evidence']))
word_index = clust_tokenizer.word_index

maxlen = 200

data6 = pad_sequences(sequences6, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data6.shape))

embedding_matrix6 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix6, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix6, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(10)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(10)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 7. Abstract - EU Call - Claim Word Embeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] + cluster_data['EU_CALL'] + cluster_data['claim']))  # Generate tokens by counting frequency
sequences7 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] + cluster_data['EU_CALL'] + cluster_data['claim']))
word_index = clust_tokenizer.word_index

maxlen = 200

data7 = pad_sequences(sequences7, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data7.shape))

embedding_matrix7 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix7, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(8,16))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix7, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(10)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(10)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 8. Abstract - EU Call - Claim/Evidence Word Emebeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] + cluster_data['EU_CALL'] + cluster_data['claim_evidence']))  # Generate tokens by counting frequency
sequences8 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] + cluster_data['EU_CALL'] + cluster_data['claim_evidence']))
word_index = clust_tokenizer.word_index

maxlen = 200

data8 = pad_sequences(sequences8, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data8.shape))

embedding_matrix8 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix8, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix8, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(10)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(10)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 9. Abstract - Project Objective - EU Call - Claim Word Embeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] +cluster_data['project_objective'] + cluster_data['EU_CALL'] + cluster_data['claim']))  # Generate tokens by counting frequency
sequences9 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] +cluster_data['project_objective'] + cluster_data['EU_CALL'] + cluster_data['claim']))
word_index = clust_tokenizer.word_index

maxlen = 200

data9 = pad_sequences(sequences9, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data9.shape))

embedding_matrix9 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix9, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix9, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(9)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(9)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

"""> 10. Abstract - Project Objective - EU Call - Claim/Evidence Word Embeddings Clustering"""

clust_tokenizer.fit_on_texts((cluster_data['abstract'] +cluster_data['project_objective']))  # Generate tokens by counting frequency
sequences10 = clust_tokenizer.texts_to_sequences((cluster_data['abstract'] +cluster_data['project_objective']))
word_index = clust_tokenizer.word_index

maxlen = 200

data10 = pad_sequences(sequences10, maxlen=maxlen)

#We have 25K, 200 word sequences now
print('New data shape: {}'.format(data10.shape))

embedding_matrix10 = create_embeddings_matrix(emb_index=embeddings_index,
                                            tokenizer=clust_tokenizer,
                                            emb_dim=embedding_dim)

X, y =  embedding_matrix10, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(4,12))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

X, y = embedding_matrix10, embeddings_index

# Instantiate the clustering model and visualizer
model = KMeans(9)
visualizer = InterclusterDistance(model)

visualizer.fit(X)        # Fit the data to the visualizer
visualizer

model = KMeans(10)
visualizer = SilhouetteVisualizer(model, colors='yellowbrick' )

visualizer.fit(X)        # Fit the data to the visualizer
visualizer











